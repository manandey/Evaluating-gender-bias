{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Predictions.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZQdBlleYgrGk"
      },
      "source": [
        "from transformers import BertTokenizer, BertModel, BertConfig, BertForSequenceClassification\n",
        "from transformers import BartTokenizer, BartModel, BartConfig, BartForSequenceClassification\n",
        "from transformers import RobertaTokenizer, RobertaModel, RobertaConfig, RobertaForSequenceClassification\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, precision_score, recall_score, precision_recall_curve\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uczOhxwnszY_",
        "outputId": "b82945ec-4fb1-4d94-9a63-6b1c6fb4777f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYJ-rgpsth8n"
      },
      "source": [
        "**Model loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dJGc-EoBiQZ"
      },
      "source": [
        "MODEL_TYPE    = 'bert-base-uncased'\n",
        "tokenizer_b     = BertTokenizer.from_pretrained(MODEL_TYPE)\n",
        "model_b = BertForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels = 3)\n",
        "model_b.load_state_dict(torch.load('/content/drive/My Drive/Gender_Bias_NLI/Models/MNLI_BERT.bin', map_location=torch.device('cpu')))\n",
        "model_b = model_ba.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07pgQFcIkMGz"
      },
      "source": [
        "\n",
        "MODEL_TYPE    = 'facebook/bart-base'\n",
        "tokenizer_ba     = BartTokenizer.from_pretrained(MODEL_TYPE)\n",
        "model_ba = BartForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels = 3)\n",
        "model_ba.load_state_dict(torch.load('/content/drive/My Drive/Gender Bias NLI Final/Models/MNLI_BART.bin', map_location=torch.device('cpu')))\n",
        "model_ba = model_ba.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xV0b43E_qvLV"
      },
      "source": [
        "MODEL_TYPE    = 'roberta-base'\n",
        "tokenizer_r     = RobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
        "model_r = RobertaForSequenceClassification.from_pretrained(MODEL_TYPE, num_labels = 3)\n",
        "model_r.load_state_dict(torch.load('/content/drive/My Drive/Gender_Bias_NLI/Models/MNLI_Roberta.bin', map_location=torch.device('cpu')))\n",
        "model_r = model_r.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr-tNoVHtlCm"
      },
      "source": [
        "**Evaluation set loading**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amgfog6El7gr"
      },
      "source": [
        "df = pd.read_csv('/content/drive/My Drive/Gender Bias NLI Final/Datasets/MNLI_Evaluation_Set.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UzNJbEMBxGwf"
      },
      "source": [
        "def get_predictions(s1, s2, model, tokenizer):\n",
        "  encoded_seq= tokenizer.encode_plus(\n",
        "    s1,\n",
        "    s2,\n",
        "    max_length=180,\n",
        "    add_special_tokens=True,\n",
        "    return_token_type_ids=True,\n",
        "    pad_to_max_length=True,\n",
        "    return_attention_mask=True,\n",
        "    return_tensors='pt',\n",
        "    truncation=True\n",
        "  )\n",
        "  input_ids = encoded_seq['input_ids'].to(device)\n",
        "  attention_mask = encoded_seq['attention_mask'].to(device)\n",
        "  # token_type_ids= encoded_seq['token_type_ids'].to(device)\n",
        "\n",
        "\n",
        "  output = model(\n",
        "        input_ids,\n",
        "        attention_mask,\n",
        "        # token_type_ids\n",
        "        \n",
        "      )\n",
        "\n",
        "  logit = output[0]\n",
        "  entail_contradiction_logits = logit[:,[0,2]]\n",
        "  probs = entail_contradiction_logits.softmax(dim=1)\n",
        "\n",
        "  entail_prob = probs[:,0].item()*100\n",
        "  cont_prob = probs[:,1].item()*100\n",
        "\n",
        "  entail_contradiction_logits = entail_contradiction_logits.detach().cpu().numpy()\n",
        "  prediction = np.argmax(entail_contradiction_logits, axis=1)\n",
        "\n",
        "\n",
        "  return prediction, entail_prob, cont_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bGEd0f9J-5d9"
      },
      "source": [
        "predictions_1 = []\n",
        "predictions_2 = []\n",
        "\n",
        "e_1 = []\n",
        "e_2 = []\n",
        "\n",
        "c_1 = []\n",
        "c_2 = []\n",
        "\n",
        "for i,r in df.iterrows(): \n",
        "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_f'], model_r, tokenizer_r)\n",
        "  predictions_1.append(prediction)\n",
        "  e_1.append(e)\n",
        "  c_1.append(c)\n",
        "\n",
        "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_m'], model_r, tokenizer_r)\n",
        "  # print(e,n,c)\n",
        "  predictions_2.append(prediction)\n",
        "  e_2.append(e)\n",
        "  c_2.append(c)\n",
        "  if np.sum(e_1)>np.sum(e_2):\n",
        "    print(np.sum(e_1)-np.sum(e_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOzGqzkCpuZy"
      },
      "source": [
        "df['f_r_pred'] = predictions_1\n",
        "df['m_r_pred'] = predictions_2\n",
        "\n",
        "df['f_r_0'] = e_1\n",
        "df['m_r_0'] = e_2\n",
        "\n",
        "df['f_r_1'] = c_1\n",
        "df['m_r_1'] = c_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe4O6edDqSha"
      },
      "source": [
        "predictions_1 = []\n",
        "predictions_2 = []\n",
        "\n",
        "e_1 = []\n",
        "e_2 = []\n",
        "\n",
        "c_1 = []\n",
        "c_2 = []\n",
        "\n",
        "for i,r in df.iterrows(): \n",
        "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_f'], model_a, tokenizer_a)\n",
        "  predictions_1.append(prediction)\n",
        "  e_1.append(e)\n",
        "  c_1.append(c)\n",
        "\n",
        "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_m'], model_a, tokenizer_a)\n",
        "  # print(e,n,c)\n",
        "  predictions_2.append(prediction)\n",
        "  e_2.append(e)\n",
        "  c_2.append(c)\n",
        "  if np.sum(e_1)>np.sum(e_2):\n",
        "    print(np.sum(e_1)-np.sum(e_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XYKyRGSq0EYJ"
      },
      "source": [
        "df['f_a_pred'] = predictions_1\n",
        "df['m_a_pred'] = predictions_2\n",
        "\n",
        "df['f_a_0'] = e_1\n",
        "df['m_a_0'] = e_2\n",
        "\n",
        "df['f_a_1'] = c_1\n",
        "df['m_a_1'] = c_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1da1I7VVnou"
      },
      "source": [
        "predictions_1 = []\n",
        "predictions_2 = []\n",
        "\n",
        "e_1 = []\n",
        "e_2 = []\n",
        "\n",
        "c_1 = []\n",
        "c_2 = []\n",
        "\n",
        "for i,r in df.iterrows(): \n",
        "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_f'], model_ba, tokenizer_ba)\n",
        "  predictions_1.append(prediction)\n",
        "  e_1.append(e)\n",
        "  c_1.append(c)\n",
        "\n",
        "  prediction, e, c = get_predictions(r['sentence1'],r['hypothesis_m'], model_ba, tokenizer_ba)\n",
        "  # print(e,n,c)\n",
        "  predictions_2.append(prediction)\n",
        "  e_2.append(e)\n",
        "  c_2.append(c)\n",
        "  if np.sum(e_1)>np.sum(e_2):\n",
        "    print(np.sum(e_1)-np.sum(e_2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD1e6Xa1V_7H"
      },
      "source": [
        "df['f_ba_pred'] = predictions_1\n",
        "df['m_ba_pred'] = predictions_2\n",
        "\n",
        "df['f_ba_0'] = e_1\n",
        "df['m_ba_0'] = e_2\n",
        "\n",
        "df['f_ba_1'] = c_1\n",
        "df['m_ba_1'] = c_2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Se_qWJt0IOe"
      },
      "source": [
        "df.to_csv('/content/drive/My Drive/Gender Bias NLI Final/Results/MNLI_Predictions_Final.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}