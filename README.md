# Evaluating Gender Bias in NLI models

## Introduction

This is a submission for NeurIPS 2020 Workshop on Dataset Curation and Security. The central theme of this work is to evaluate the gender bias in natural language understanding through inference. The authors propose an evaluation methodology to measure these biases by constructing a challenge task which involves pairing gender neutral premise against gender-specific hypothesis. The results suggest that three models (BERT, RoBERTa, BART) trained on MNLI and SNLI data-sets are significantly prone to gender-induced prediction errors and debiasing techniques such as data augmentation does mitigate this bias in certain cases.
